{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pongs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pongs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pongs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined text preprocessing and cleaning transformer\n",
    "class TextPreprocessorCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def remove_emoji(self, text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    def clean_and_process(self, text):\n",
    "        text = re.sub(r\"<[^>]+>\", \"\", text)  # Remove HTML tags\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "        text = text.strip()  # Remove extra spaces\n",
    "        text = self.remove_emoji(text)  # Remove emojis\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "\n",
    "        clean_text = [word for word in text.split() if word not in self.stop_words]\n",
    "        lemmatized_text = [self.lemmatizer.lemmatize(word, 'v') for word in clean_text]\n",
    "\n",
    "        return ' '.join(lemmatized_text)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self.clean_and_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22.5 s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 21.6 s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Ensure the column names are correct\n",
    "X = df[\"Review\"]  # Adjust if needed\n",
    "Y = df[\"Sentiment\"]  # Adjust if needed\n",
    "\n",
    "# Split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Apply undersampling\n",
    "undersampler = RandomUnderSampler()\n",
    "x_train_resampled, y_train_resampled = undersampler.fit_resample(x_train.to_frame(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817174960390946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.81      0.77    168023\n",
      "     Neutral       0.34      0.62      0.44     79971\n",
      "    Positive       0.96      0.84      0.90    679193\n",
      "\n",
      "    accuracy                           0.82    927187\n",
      "   macro avg       0.68      0.76      0.70    927187\n",
      "weighted avg       0.87      0.82      0.84    927187\n",
      "\n",
      "CPU times: total: 3min 40s\n",
      "Wall time: 9min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Encode the target labels (y) using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_resampled)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Create the pipeline with preprocessing and classifier\n",
    "pipeline = make_pipeline(\n",
    "    TextPreprocessorCleaner(),  # Custom text preprocessing\n",
    "    TfidfVectorizer(),          # TF-IDF vectorization\n",
    "    LinearSVC(C=0.1)            # SVM classifier\n",
    ")\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(x_train_resampled.squeeze(), y_train_encoded)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "# Decode the predicted labels back to original form\n",
    "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Print classification report\n",
    "print(accuracy_score(y_pred,y_test_encoded) * 100)\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pipeline and label encoder using joblib\n",
    "joblib.dump(pipeline, 'sentiment_pipeline.joblib')\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import gradio as gr\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Define the TextPreprocessorCleaner class\n",
    "class TextPreprocessorCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def remove_emoji(self, text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    def clean_and_process(self, text):\n",
    "        text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.strip()\n",
    "        text = self.remove_emoji(text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        clean_text = [word for word in text.split() if word not in self.stop_words]\n",
    "        lemmatized_text = [self.lemmatizer.lemmatize(word, 'v') for word in clean_text]\n",
    "\n",
    "        return ' '.join(lemmatized_text)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, str):\n",
    "            return self.clean_and_process(X)\n",
    "        elif isinstance(X, list):\n",
    "            return [self.clean_and_process(text) for text in X]\n",
    "        else:\n",
    "            raise TypeError(\"Input data should be a string or a list of strings.\")\n",
    "\n",
    "\n",
    "# Load the trained model and label encoder\n",
    "pipeline = joblib.load('sentiment_pipeline.joblib')\n",
    "label_encoder = joblib.load('label_encoder.joblib')\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    # Process the input text\n",
    "    processed_text = pipeline.named_steps['textpreprocessorcleaner'].transform(text)\n",
    "    if processed_text.strip() == \"\":\n",
    "        return \"Error: Processed text is empty. Please provide valid input.\"\n",
    "\n",
    "    # Predict sentiment\n",
    "    prediction = pipeline.predict([processed_text])\n",
    "    \n",
    "    # Decode the prediction to original labels\n",
    "    decoded_prediction = label_encoder.inverse_transform(prediction)\n",
    "    return decoded_prediction[0]\n",
    "\n",
    "# Create the Gradio app with Blocks\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Sentiment Analysis From Software Review Text\")\n",
    "    gr.Markdown(\"Your Review Of this Software\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text_input = gr.Textbox(lines=2, placeholder=\"Enter your text here...\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(label=\"Sentiment\")\n",
    "\n",
    "    # Define the function to be called when the button is clicked\n",
    "    submit_button.click(fn=predict_sentiment, inputs=text_input, outputs=output_text)\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue(True).launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pongs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pongs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7862 <> https://c488c6e61669309351.gradio.live\n",
      "Killing tunnel 127.0.0.1:7863 <> https://d23dcffb12086ed528.gradio.live\n",
      "Killing tunnel 127.0.0.1:7864 <> https://31fd72e0f0ccf21b5e.gradio.live\n"
     ]
    }
   ],
   "source": [
    "%run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "gradio==3.30.0\n",
    "joblib\n",
    "nltk\n",
    "scikit-learn\n",
    "pandas\n",
    "imblearn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
